# Daily Updates on 3D-Related Papers

This repository automatically fetches new or updated arXiv papers in the [cs.CV] category every day, checks if they are relevant to "3D reconstruction" or "3D generation" via ChatGPT, and lists them below.

## How It Works
1. A GitHub Actions workflow runs daily at 09:00 UTC.  
2. It uses the script [fetch_cv_3d_papers.py](fetch_cv_3d_papers.py) to:  
   - Retrieve the latest arXiv papers in cs.CV.  
   - Use ChatGPT to filter out those related to 3D reconstruction/generation.  
   - Update this README.md with the new findings.  
   - Send an email via 163 Mail if any relevant papers are found.  

# Paper List
## Arxiv 2025-01-31

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2501.17978v2 VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting](http://arxiv.org/abs/2501.17978v2) | 3D generation 3D生成 | 3D Gaussian Splatting<br>view-dependent representation<br>3D高斯渲染<br>视角依赖表示 | input: images 图片<br>extend the 3D Gaussian Splatting model 扩展3D高斯渲染模型<br>introduce an additional symmetric matrix 引入额外的对称矩阵<br>achieve view-dependent opacity representation 实现视角依赖的透明度表示<br>output: improved 3D scene reconstruction 输出：改进的3D场景重建 |
8.5 | [[8.5] 2501.19319v1 Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven Surface Normal-aware Tracking and Mapping](http://arxiv.org/abs/2501.19319v1) | 3D reconstruction 三维重建 | 3D reconstruction<br>3D Gaussian Splatting<br>endoscopic SLAM<br>depth reconstruction<br>三维重建<br>3D高斯斑点<br>内窥镜SLAM<br>深度重建 | input: endoscopic image sequences 内窥镜图像序列<br>Step 1: tracking using Gaussian Splatting 使用高斯斑点的跟踪<br>Step 2: mapping and bundle adjustment 映射与束调整<br>Step 3: surface normal-aware reconstruction 结合表面法向量进行重构<br>output: accurate 3D reconstruction and real-time tracking 输出: 精确的3D重建与实时跟踪 |
8.5 | [[8.5] 2501.19270v1 Imagine with the Teacher: Complete Shape in a Multi-View Distillation Way](http://arxiv.org/abs/2501.19270v1) | 3D reconstruction  三维重建 | Point Cloud Completion<br>3D Shape Completion<br>Knowledge Distillation<br>Points Completion<br>点云补全<br>3D形状补全<br>知识蒸馏<br>点补全 | input: incomplete point cloud  有缺失的点云<br>step1: apply autoencoder to encode the point cloud  应用自编码器对点云进行编码<br>step2: use knowledge distillation for completion  使用知识蒸馏进行补全<br>step3: output: completed 3D shape  输出：完整的3D形状 |
8.5 | [[8.5] 2501.19196v1 RaySplats: Ray Tracing based Gaussian Splatting](http://arxiv.org/abs/2501.19196v1) | 3D generation 3D生成 | 3D Gaussian Splatting<br>Gaussian Splatting<br>3D高斯喷溅<br>高斯喷溅 | Input: 2D images 2D图像<br>Ray-tracing mechanism 射线追踪机制<br>Intersection computation 交点计算<br>Ray-tracing algorithms construction 射线追踪算法构建<br>Final 3D object with lighting and shadows 最终带有光影效果的三维物体 |
8.5 | [[8.5] 2501.19088v1 JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting](http://arxiv.org/abs/2501.19088v1) | 3D generation 3D生成 | 3D Gaussian Splatting<br>3D reconstruction<br>实时渲染<br>3D高斯分喷<br>三维重建 | input: 3D key points (输入：3D关键点)<br>Step 1: Create a joint-driven 3D Gaussian representation (步骤1：创建联合驱动的3D高斯表示)<br>Step 2: Implement differentiable spatial transformations (步骤2：实现可微分的空间变换)<br>Step 3: Apply real-time shadow simulation method (步骤3：应用实时阴影模拟方法)<br>output: High-fidelity hand images (输出：高保真的手部图像) |
8.5 | [[8.5] 2501.18982v1 OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation](http://arxiv.org/abs/2501.18982v1) | 3D generation 3D生成 | 3D generation<br>3D gaussian<br>物体生成<br>3D高斯 | input: 3D assets 3D资产<br>extract: physical properties 提取物理属性<br>generate: physics-based dynamics 生成基于物理的动态<br>output: dynamic scene 输出动态场景 |
7.5 | [[7.5] 2501.19382v1 LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention Networks](http://arxiv.org/abs/2501.19382v1) | Autonomous Driving 自动驾驶 | LiDAR<br>loop closure detection<br>graph attention networks<br>place recognition<br>semanitic registration<br>激光雷达<br>回环闭合检测<br>图注意力网络<br>地点识别<br>语义注册 | input: semantic graphs 语义图<br>step1: encode semantic graphs using graph attention networks 使用图注意力网络编码语义图<br>step2: compare graph vectors to identify loop closure 比较图向量以识别回环闭合<br>step3: estimate 6 DoF pose constraint using semantic registration 使用语义注册估计6自由度位姿约束<br>output: loop closure detection results 回环闭合检测结果 |
7.5 | [[7.5] 2501.19259v1 Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for Autonomous Drone FlighT at the Edge](http://arxiv.org/abs/2501.19259v1) | Autonomous Driving 自主驾驶 | Autonomous Driving<br>Neuromorphic Vision<br>Real-time Navigation<br>Autonomous Systems<br>自驾驶<br>神经形态视觉<br>实时导航<br>自主系统 | Input: Human speech commands 人类语音指令<br>Step 1: Translate speech into planning commands 将语音翻译成规划指令<br>Step 2: Execute commands using neuromorphic vision 执行命令使用神经形态视觉<br>Step 3: Navigate and avoid obstacles in real-time 实时导航和避免障碍<br>Output: Autonomous drone navigation output 自主无人机导航输出 |
7.5 | [[7.5] 2501.19252v1 Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search](http://arxiv.org/abs/2501.19252v1) | Video Generation 视频生成 | video generation<br>text-to-video models<br>视频生成<br>文本到视频模型 | input: diffusion model inputs 输入：扩散模型输入<br>step1: align video frames with text prompts 步骤1：将视频帧与文本提示对齐<br>step2: utilize a beam search strategy to optimize output 使用束搜索策略优化输出<br>step3: compute metrics for perceptual quality evaluation 计算感知质量评估的指标<br>output: high-quality, aligned video generation 输出：高质量、对齐的视频生成 |
7.5 | [[7.5] 2501.19035v1 SynthmanticLiDAR: A Synthetic Dataset for Semantic Segmentation on LiDAR Imaging](http://arxiv.org/abs/2501.19035v1) | Autonomous Driving 自动驾驶 | Semantic Segmentation<br>LiDAR Imaging<br>Autonomous Driving<br>合成分割<br>LiDAR成像<br>自动驾驶 | input: LiDAR data 输入: LiDAR 数据<br>step1: generate synthetic dataset 生成合成数据集<br>step2: utilize CARLA simulator 使用 CARLA 模拟器<br>step3: train segmentation algorithms 训练分割算法<br>output: improved segmentation performance 输出: 改进的分割性能 |
7.5 | [[7.5] 2501.17159v2 IC-Portrait: In-Context Matching for View-Consistent Personalized Portrait](http://arxiv.org/abs/2501.17159v2) | Image Generation 图像生成 | personalized portrait generation<br>identity preservation<br>view-consistent reconstruction<br>个性化肖像生成<br>身份保留<br>视角一致重建 | input: reference images 参考图像<br>step1: Lighting-Aware Stitching 光照感知拼接<br>step2: View-Consistent Adaptation 视角一致自适应<br>step3: ControlNet-like supervision 控制网络样监督<br>output: personalized portraits 个性化肖像 |
6.5 | [[6.5] 2501.18994v1 VKFPos: A Learning-Based Monocular Positioning with Variational Bayesian Extended Kalman Filter Integration](http://arxiv.org/abs/2501.18994v1) | Autonomous Driving (自动驾驶) | Monocular Positioning<br>Extended Kalman Filter<br>Deep Learning<br>Single-shot<br>单目定位<br>扩展卡尔曼滤波<br>深度学习<br>单次 | input: monocular images 单目图像<br>step1: Absolute Pose Regression (APR) 绝对姿态回归<br>step2: Relative Pose Regression (RPR) 相对姿态回归<br>step3: Integrate APR and RPR using EKF 通过扩展卡尔曼滤波整合APR和RPR<br>output: accurate positioning results 精确定位结果 |
6.0 | [[6.0] 2501.19331v1 Consistent Video Colorization via Palette Guidance](http://arxiv.org/abs/2501.19331v1) | Video Generation 视频生成 | Video Colorization<br>Stable Video Diffusion<br>Palette Guidance<br>视频上色<br>稳定视频扩散<br>调色板引导 | input: video sequences 视频序列<br>step 1: design palette-based color guider 设计调色板引导器<br>step 2: utilize Stable Video Diffusion as base model 利用稳定视频扩散作为基础模型<br>step 3: generate vivid colors using color context 根据颜色上下文生成生动的颜色<br>output: colorized video sequences 上色的视频序列 |
5.5 | [[5.5] 2501.18865v1 REG: Rectified Gradient Guidance for Conditional Diffusion Models](http://arxiv.org/abs/2501.18865v1) | Image Generation 图像生成 | conditional generation<br>diffusion models<br>conditional generation 条件生成<br>扩散模型 | input: guidance techniques 指导技术<br>step1: replace the scaled marginal distribution target 替换缩放的边际分布目标<br>step2: implement rectified gradient guidance 实施矩形梯度指导<br>step3: conduct experiments on image generation tasks 进行图像生成任务的实验<br>output: improved image generation results 改进的图像生成结果 |


## Arxiv 2025-01-31

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2501.19196v1 RaySplats: Ray Tracing based Gaussian Splatting](http://arxiv.org/abs/2501.19196v1) | 3D generation 三维生成 | 3D Gaussian Splatting<br>Ray Tracing<br>3D高斯点云<br>光线追踪 | input: 2D images 2D图像<br>process: Gaussian Splatting 高斯点云渲染<br>process: ray tracing based on Gaussian primitives 基于高斯原始体的光线追踪<br>output: 3D objects with light and shadow effects 输出具有光影效果的3D物体 |
9.0 | [[9.0] 2501.17978v2 VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting](http://arxiv.org/abs/2501.17978v2) | 3D generation 3D生成 | 3D Gaussian Splatting<br>view-dependent rendering<br>3D高斯点云<br>视角依赖的渲染 | input: 3D scene reconstruction from images 3D场景重建从图像中提取<br>step 1: extend 3D Gaussian Splatting model 扩展3D高斯点云模型<br>step 2: introduce symmetric matrix to enhance opacity representation 引入对称矩阵以增强不透明性表示<br>step 3: optimize suppression of Gaussians based on viewer perspective 根据观察者视角优化高斯的抑制<br>output: improved representation of view-dependent reflections and specular highlights 输出：改进视角依赖的反射和镜面高光的表示 |
8.5 | [[8.5] 2501.19319v1 Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven Surface Normal-aware Tracking and Mapping](http://arxiv.org/abs/2501.19319v1) | 3D reconstruction  三维重建 | 3D Gaussian Splatting<br>SLAM<br>endoscopic reconstruction<br>depth reconstruction<br>3D 高斯点<br>SLAM<br>内窥镜重建<br>深度重建 | input: endoscopic images 内窥镜图像<br>step1: surface normal-aware tracking 表面法线感知跟踪<br>step2: accurate mapping 精确地图构建<br>step3: bundle adjustment 捆绑调整<br>output: geometrically accurate 3D reconstruction 准确的三维重建 |
8.5 | [[8.5] 2501.19252v1 Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search](http://arxiv.org/abs/2501.19252v1) | Video Generation 视频生成 | Text-to-video<br>Diffusion models<br>Video generation<br>评分调整<br>文本转视频<br>扩散模型<br>视频生成<br>奖励校准 | input: video generation prompts 视频生成提示<br>step1: employ diffusion latent beam search 使用扩散潜在光束搜索<br>step2: maximize alignment reward 最大化对齐奖励<br>step3: improve perceptual quality 提升感知质量<br>output: high-quality video optimized for natural movement 输出：高质量视频，优化自然运动 |
8.5 | [[8.5] 2501.19088v1 JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting](http://arxiv.org/abs/2501.19088v1) | 3D generation 3D生成 | 3D Gaussian Splatting<br>animatable hand avatar<br>3D高斯喷涂<br>可动画手部化身 | input: 3D key points 3D关键点<br>Jointly 3D Gaussian Splatting (3DGS) joint-driven representation 联合3D高斯喷涂（3DGS）驱动表示<br>apply spatial transformations based on 3D key points 基于3D关键点应用空间变换<br>real-time rendering and shadow simulation 实时渲染和阴影模拟<br>output: animatable high-fidelity hand images 输出：可动画的高保真手部图像 |
8.5 | [[8.5] 2501.18982v1 OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation](http://arxiv.org/abs/2501.18982v1) | 3D generation 3D生成 | 3D generation<br>3D gaussian<br>3D生成<br>3D高斯 | input: user-specified prompts 用户指定的提示<br>step1: define a scene according to user prompts 根据用户提示定义场景<br>step2: estimate material weighting factors using a pretrained video diffusion model 使用预训练的视频扩散模型估计材料权重因子<br>step3: represent each 3D asset as a collection of constitutive 3D Gaussians 将每个3D资产表示为一组组成的3D高斯分布<br>output: a physics-based 3D dynamic scene 输出：基于物理的3D动态场景 |
8.0 | [[8.0] 2501.19270v1 Imagine with the Teacher: Complete Shape in a Multi-View Distillation Way](http://arxiv.org/abs/2501.19270v1) | 3D reconstruction三维重建 | Point Cloud Completion<br>Multi-view Distillation<br>3D Shape Recovery<br>点云补全<br>多视图蒸馏<br>3D形状恢复 | input: incomplete point cloud 输入: 不完整的点云<br>step1: apply autoencoder architecture 应用自编码器架构<br>step2: use knowledge distillation strategy to enhance completion 使用知识蒸馏策略以增强完成度<br>step3: output: completed point cloud 输出: 完整的点云 |
7.5 | [[7.5] 2501.19382v1 LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention Networks](http://arxiv.org/abs/2501.19382v1) | Autonomous Driving 自主驾驶 | Loop Closure Detection<br>Semantic Graphs<br>Graph Attention Networks<br>闭环检测<br>语义图<br>图注意力网络 | input: point cloud 输入: 点云<br>step1: encode semantic graphs using graph attention networks 步骤1: 使用图注意力网络编码语义图<br>step2: generate graph vectors through self-attention mechanisms 步骤2: 通过自注意力机制生成图向量<br>step3: compare graph vectors to detect loop closure 步骤3: 比较图向量以检测闭环<br>output: loop closure candidates 输出: 闭环候选 |
7.5 | [[7.5] 2501.19035v1 SynthmanticLiDAR: A Synthetic Dataset for Semantic Segmentation on LiDAR Imaging](http://arxiv.org/abs/2501.19035v1) | Autonomous Driving 自主驾驶 | Semantic segmentation<br>LiDAR imaging<br>autonomous driving<br>合成分割<br>LiDAR成像<br>自主驾驶 | input: LiDAR images (输入: LiDAR图像)<br>modify CARLA simulator (修改CARLA模拟器)<br>generate SynthmanticLiDAR dataset (生成SynthmanticLiDAR数据集)<br>evaluate with transfer learning (使用迁移学习进行评估)<br>output: improved semantic segmentation performance (输出: 改进的语义分割性能) |
7.5 | [[7.5] 2501.17159v2 IC-Portrait: In-Context Matching for View-Consistent Personalized Portrait](http://arxiv.org/abs/2501.17159v2) | Image Generation 图像生成 | Personalized Portrait Generation<br>3D-aware relighting<br>个性化肖像生成<br>具3D感知的重光照 | Input: reference portrait images 参考肖像图像<br>Step 1: Lighting-Aware Stitching 具光照感知的拼接<br>Step 2: View-Consistent Adaptation 具视图一致的适配<br>Output: personalized portraits with identity preservation 具有身份保留的个性化肖像 |
7.0 | [[7.0] 2501.19243v1 Accelerating Diffusion Transformer via Error-Optimized Cache](http://arxiv.org/abs/2501.19243v1) | Image Generation 图像生成 | Image Generation<br>Diffusion Transformer<br>ImageNet Dataset<br>图像生成<br>扩散变换器<br>ImageNet数据集 | input: Diffusion Transformer features (扩散变换器特征)<br>extract caching differences (提取缓存差异)<br>optimize cache based on errors (基于错误优化缓存)<br>output: improved generated images (输出: 改进的生成图像) |
6.5 | [[6.5] 2501.19259v1 Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for Autonomous Drone FlighT at the Edge](http://arxiv.org/abs/2501.19259v1) | Autonomous Driving 自主驾驶 | autonomous driving<br>natural language processing<br>neuroscience<br>autonomous navigation<br>自主驾驶<br>自然语言处理<br>神经科学<br>自主导航 | input: human speech and dynamic environment 输入：人类语言和动态环境<br>step1: translate human speech into planning commands 步骤1：将人类语言翻译为规划命令<br>step2: navigate and avoid obstacles using neuromorphic vision 步骤2：利用神经形态视觉导航并避免障碍物<br>output: real-time autonomous navigation output 实时自主导航结果 |
6.5 | [[6.5] 2501.18994v1 VKFPos: A Learning-Based Monocular Positioning with Variational Bayesian Extended Kalman Filter Integration](http://arxiv.org/abs/2501.18994v1) | Autonomous Driving 自主驾驶 | monocular positioning<br>extended kalman filter<br>variational bayesian inference<br>单目定位<br>扩展卡尔曼滤波<br>变分贝叶斯推理 | input: monocular images 单目图像<br>step1: Absolute Pose Regression (APR) 绝对姿态回归<br>step2: Relative Pose Regression (RPR) 相对姿态回归<br>step3: Integration with Extended Kalman Filter (EKF) 通过扩展卡尔曼滤波整合<br>output: accurate positional predictions 准确的位置信息预测 |


## Arxiv 2025-01-30

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
8.5 | [[8.5] 2501.18594v1 Foundational Models for 3D Point Clouds: A Survey and Outlook](http://arxiv.org/abs/2501.18594v1) | 3D reconstruction 3D重建 | 3D point clouds<br>foundational models<br>3D视觉理解<br>基础模型<br>3D点云 | input: 3D point clouds 3D点云<br>step1: review of foundational models FMs 基础模型的回顾<br>step2: categorize use of FMs in 3D tasks 分类基础模型在3D任务中的应用<br>step3: summarize state-of-the-art methods 总结最新的方法<br>output: comprehensive overview of FMs for 3D understanding 输出：基础模型在3D理解中的综合概述 |
8.5 | [[8.5] 2501.18162v1 IROAM: Improving Roadside Monocular 3D Object Detection Learning from Autonomous Vehicle Data Domain](http://arxiv.org/abs/2501.18162v1) | Autonomous Driving 自动驾驶 | 3D object detection<br>autonomous driving<br>3D对象检测<br>自动驾驶 | input: roadside data and vehicle-side data<br>In-Domain Query Interaction module learns content and depth information<br>Cross-Domain Query Enhancement decouples queries into semantic and geometry parts<br>outputs enhanced object queries |
8.5 | [[8.5] 2501.18110v1 Lifelong 3D Mapping Framework for Hand-held & Robot-mounted LiDAR Mapping Systems](http://arxiv.org/abs/2501.18110v1) | 3D reconstruction 三维重建 | 3D Mapping<br>3D Reconstruction<br>Lifelong Mapping<br>激光雷达<br>三维映射<br>三维重建<br>终身映射 | Input: Hand-held and robot-mounted LiDAR maps 输入：手持和机器人安装的激光雷达地图<br>Dynamic point removal algorithm 动态点去除算法<br>Multi-session map alignment using feature descriptor matching and fine registration 多会话地图对齐，使用特征描述符匹配和精细配准<br>Map change detection to identify changes between aligned maps 地图变化检测以识别对齐地图之间的变化<br>Map version control for maintaining current environmental state and querying changes 地图版本控制，用于维护当前环境状态和查询变化 |
8.0 | [[8.0] 2501.18595v1 ROSA: Reconstructing Object Shape and Appearance Textures by Adaptive Detail Transfer](http://arxiv.org/abs/2501.18595v1) | Mesh Reconstruction 网格重建 | Mesh Reconstruction<br>3D reconstruction<br>网格重建<br>三维重建 | input: limited set of images 限制的图像集<br>step1: optimize mesh geometry 优化网格几何形状<br>step2: refine mesh with spatially adaptive resolution 使用空间自适应分辨率细化网格<br>step3: reconstruct high-resolution textures 重新构建高分辨率纹理<br>output: textured mesh with detailed appearance 带有详细外观的纹理网格 |
7.5 | [[7.5] 2501.18590v1 DiffusionRenderer: Neural Inverse and Forward Rendering with Video Diffusion Models](http://arxiv.org/abs/2501.18590v1) | Rendering Techniques 渲染技术 | Inverse Rendering<br>Forward Rendering<br>Video Diffusion Models<br>Inverse渲染<br>正向渲染<br>视频扩散模型 | input: real-world videos, 真实世界视频<br>step1: estimate G-buffers using inverse rendering model, 使用逆向渲染模型估计G-buffer<br>step2: generate photorealistic images from G-buffers, 从G-buffer生成照片级真实图像<br>output: relit images, material edited images, realistic object insertions, 重新照明图像，材料编辑图像，逼真的物体插入 |
7.5 | [[7.5] 2501.18315v1 Surface Defect Identification using Bayesian Filtering on a 3D Mesh](http://arxiv.org/abs/2501.18315v1) | Mesh Reconstruction 网格重建 | 3D Mesh<br>Mesh Reconstruction<br>3D网格<br>网格重建 | input: CAD model and point cloud data 输入：CAD模型和点云数据<br>transform CAD model into polygonal mesh 将CAD模型转换为多边形网格<br>apply weighted least squares algorithm 应用加权最小二乘算法<br>estimate state based on point cloud measurements 根据点云测量估计状态<br>output: high-precision defect identification 输出：高精度缺陷识别 |
7.5 | [[7.5] 2501.17636v2 Efficient Interactive 3D Multi-Object Removal](http://arxiv.org/abs/2501.17636v2) | 3D reconstruction 三维重建 | 3D scene understanding<br>multi-object removal<br>3D场景理解<br>多对象移除 | input: selected areas and objects for removal 选定的移除区域和对象<br>step1: mask matching and refinement mask 匹配和细化掩码步骤<br>step2: homography-based warping 同伦变换基础的扭曲<br>step3: inpainting process 修复过程<br>output: modified 3D scene 修改后的3D场景 |
7.0 | [[7.0] 2501.18246v1 Ground Awareness in Deep Learning for Large Outdoor Point Cloud Segmentation](http://arxiv.org/abs/2501.18246v1) | 3D reconstruction  三维重建 | point cloud segmentation<br>outdoor point clouds<br>semantic segmentation<br>point cloud<br>关键点云分割<br>户外点云<br>语义分割<br>点云 | input: outdoor point clouds 户外点云<br>compute Digital Terrain Models (DTMs) 计算数字地形模型<br>employ RandLA-Net for segmentation 使用 RandLA-Net 进行分割<br>evaluate performance on datasets 评估在数据集上的表现<br>integrate relative elevation features 集成相对高程特征 |
6.5 | [[6.5] 2501.18494v1 Runway vs. Taxiway: Challenges in Automated Line Identification and Notation Approaches](http://arxiv.org/abs/2501.18494v1) | Autonomous Driving 自动驾驶 | Automated line identification 自动化线识别<br>Convolutional Neural Network 卷积神经网络<br>runway markings 跑道标记<br>autonomous systems 自动化系统<br>labeling algorithms 标记算法 | input: runway and taxiway images 跑道和滑行道图像<br>Step 1: color threshold adjustment 颜色阈值调整<br>Step 2: refine region of interest selection 精细化感兴趣区域选择<br>Step 3: integrate CNN classification 集成CNN分类<br>output: improved marking identification 改进的标记识别 |


## Newly Found Papers on ...
(Older entries get replaced automatically when the script runs again.)