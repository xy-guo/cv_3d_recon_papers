# Daily Updates on 3D-Related Papers

This repository automatically fetches new or updated arXiv papers in the [cs.CV] category every day, checks if they are relevant to "3D reconstruction" or "3D generation" via ChatGPT, and lists them below.

## How It Works
1. A GitHub Actions workflow runs daily at 09:00 UTC.  
2. It uses the script [fetch_cv_3d_papers.py](fetch_cv_3d_papers.py) to:  
   - Retrieve the latest arXiv papers in cs.CV.  
   - Use ChatGPT to filter out those related to 3D reconstruction/generation.  
   - Update this README.md with the new findings.  
   - Send an email via 163 Mail if any relevant papers are found.  

# Paper List
## Arxiv 2025-02-04

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.00173 Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation](https://arxiv.org/abs/2502.00173) <br> [{'name': 'Rohan Chacko, Nicolai Haeni, Eldar Khaliullin, Lin Sun, Douglas Lee'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D instance segmentation 3D实例分割<br>Gaussian Splatted Radiance Fields 高斯点云辐射场<br>novel view synthesis 新视图合成 | Input: Posed 2D image data 2D图像数据<br>Step1: Extract per-image 2D segmentation masks 提取每帧的2D分割掩码<br>Step2: 2D-to-3D lifting to assign unique object IDs 在3D中分配唯一对象ID的2D到3D提升流程<br>Step3: Incremental merging of object fragments into coherent objects 将对象片段合并成一致的对象<br>Output: High-quality 3D object segments 高质量的3D对象片段 |
9.5 | [[9.5] 2502.00360 Shape from Semantics: 3D Shape Generation from Multi-View Semantics](https://arxiv.org/abs/2502.00360) <br> [{'name': 'Liangchen Li, Caoliwen Wang, Yuqi Zhou, Bailin Deng, Juyong Zhang'}] | 3D Shape Generation 3D形状生成 | v2<br>3D reconstruction<br>shape generation<br>semantic input | Input: Semantic descriptions 语义描述<br>Step1: Distill 3D geometry from 2D diffusion models 从2D扩散模型提取3D几何<br>Step2: Refine textures using image and video generation models 使用图像和视频生成模型细化纹理<br>Step3: Represent the refined 3D model with neural implicit representations 使用神经隐式表示来表示细化的3D模型<br>Output: Fabricable high-quality meshes 可制造的高质量网格 |
9.5 | [[9.5] 2502.00801 Environment-Driven Online LiDAR-Camera Extrinsic Calibration](https://arxiv.org/abs/2502.00801) <br> [{'name': 'Zhiwei Huang, Jiaqi Li, Ping Zhong, Rui Fan'}] | 3D Reconstruction and Modeling 三维重建 | v2<br>LiDAR-camera calibration<br>3D reconstruction<br>autonomous driving | Input: LiDAR and camera data 激光雷达和相机数据<br>Step1: Environment interpretation 环境解读<br>Step2: Data fusion 数据融合<br>Step3: Dual-path correspondence matching 双色通道对应匹配<br>Step4: Spatial-temporal optimization 空间-时间优化<br>Output: Accurate extrinsic calibration 精准的外部标定 |
9.5 | [[9.5] 2502.01045 WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction](https://arxiv.org/abs/2502.01045) <br> [{'name': 'Zilong Wang, Zhiyang Dou, Yuan Liu, Cheng Lin, Xiao Dong, Yunhui Guo, Chenxu Zhang, Xin Li, Wenping Wang, Xiaohu Guo'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>3D reconstruction<br>generative models<br>dynamic avatars | Input: Monocular video 单目视频<br>Step1: Generative prior usage 生成优先级使用<br>Step2: Dual-Space Optimization 双空间优化<br>Step3: View selection strategy 视图选择策略<br>Step4: Pose feature injection 姿势特征注入<br>Output: High-fidelity dynamic human avatars 高保真动态人形象 |
9.5 | [[9.5] 2502.01405 FourieRF: Few-Shot NeRFs via Progressive Fourier Frequency Control](https://arxiv.org/abs/2502.01405) <br> [{'name': 'Diego Gomez, Bingchen Gong, Maks Ovsjanikov'}] | 3D Reconstruction and Modeling 三维重建与建模 | v2<br>Few-Shot NeRF<br>3D Reconstruction<br>Neural Rendering | Input: Limited input views 有限的输入视角<br>Step1: Frequency control frequency control<br>Step2: Curriculum training curriculum training<br>Step3: Scene reconstruction scene reconstruction<br>Output: Accurate 3D representations 准确的三维表示 |
9.2 | [[9.2] 2502.00262 Your submission contained main.bib and main.tex file, but no main.bbl file (include main.bbl, or submit without main.bib; and remember to verify references)](https://arxiv.org/abs/2502.00262) <br> [{'name': 'Dianwei Chen, Zifan Zhang, Yuchen Liu, Xianfeng Terry Yang'}] | Autonomous Systems and Robotics 自动驾驶 | v2<br>hazard detection<br>vision-language model<br>autonomous driving | Input: Multimodal data fusion 多模态数据融合<br>Step1: Semantic and visual inputs integration 语义和视觉输入集成<br>Step2: Supervised fine-tuning of vision-language models 有监督微调视觉语言模型<br>Step3: Hazard detection and edge case evaluation 危险检测和边缘案例评估<br>Output: Enhanced situational awareness 改进的情境意识 |
9.2 | [[9.2] 2502.00315 MonoDINO-DETR: Depth-Enhanced Monocular 3D Object Detection Using a Vision Foundation Model](https://arxiv.org/abs/2502.00315) <br> [{'name': 'Jihyeok Kim, Seongwoo Moon, Sungwon Nah, David Hyunchul Shim'}] | 3D Object Detection 3D对象检测 | v2<br>3D object detection 3D对象检测<br>monocular vision 单目视觉<br>depth estimation 深度估计 | Input: Monocular images 单目图像<br>Step1: Depth estimation using Vision Transformer 步骤1：使用视觉Transformer进行深度估计<br>Step2: Feature extraction with Hierarchical Feature Fusion 步骤2：利用层次特征融合提取特征<br>Step3: Object detection using DETR architecture 步骤3：使用DETR架构进行对象检测<br>Output: 3D bounding boxes for detected objects 输出：检测到对象的3D边界框 |
8.5 | [[8.5] 2502.00074 SpikingRTNH: Spiking Neural Network for 4D Radar Object Detection](https://arxiv.org/abs/2502.00074) <br> [{'name': 'Dong-Hee Paek, Seung-Hyun Kong'}] | 3D Object Detection 目标检测 | v2<br>4D Radar<br>3D object detection<br>energy efficiency<br>autonomous driving | Input: 4D Radar point clouds 4D雷达点云<br>Step1: Convert RTNH to SNN architecture 将RTNH转换为SNN架构<br>Step2: Implement biological top-down inference (BTI) 实现生物学自上而下推理(BTI)<br>Step3: Model evaluation and comparison 模型评估与比较<br>Output: Energy-efficient 3D object detection model 能源高效的3D目标检测模型 |
8.5 | [[8.5] 2502.00342 Embodied Intelligence for 3D Understanding: A Survey on 3D Scene Question Answering](https://arxiv.org/abs/2502.00342) <br> [{'name': 'Zechuan Li, Hongshan Yu, Yihao Ding, Yan Li, Yong He, Naveed Akhtar'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>3D Scene Question Answering<br>multimodal models | Input: 3D scene representation and query 3D场景表示和查询<br>Step1: Systematic literature review 系统文献综述<br>Step2: Dataset analysis 数据集分析<br>Step3: Methodology evaluation 方法评估<br>Output: Comprehensive insights and challenges on 3D SQA 对3D SQA的综合见解和挑战 |
8.5 | [[8.5] 2502.00500 Video Latent Flow Matching: Optimal Polynomial Projections for Video Interpolation and Extrapolation](https://arxiv.org/abs/2502.00500) <br> [{'name': 'Yang Cao, Zhao Song, Chiwun Yang'}] | Video Generation 视频生成 | v2<br>video generation<br>interpolation<br>extrapolation<br>latent flow matching | Input: Video frames 视频帧<br>Step1: Model latent flow 模型潜在流<br>Step2: Polynomial projection 多项式投影<br>Step3: Generate time-dependent frames 生成时间相关帧<br>Output: Video with interpolation and extrapolation 带插值和外推的视频 |
8.5 | [[8.5] 2502.00708 PhiP-G: Physics-Guided Text-to-3D Compositional Scene Generation](https://arxiv.org/abs/2502.00708) <br> [{'name': 'Qixuan Li, Chao Wang, Zongjin He, Yan Peng'}] | 3D Generation 三维生成 | v2<br>3D generation<br>compositional scenes<br>large language models | Input: Complex scene descriptions 复杂场景描述<br>Step1: Semantic parsing and relationship extraction 语义解析和关系提取<br>Step2: Scene graph generation 场景图生成<br>Step3: 2D and 3D asset generation 2D和3D资产生成<br>Step4: Layout prediction and planning 布局预测与规划<br>Output: High-quality 3D compositional scenes 高质量三维组合场景 |
8.5 | [[8.5] 2502.00843 VLM-Assisted Continual learning for Visual Question Answering in Self-Driving](https://arxiv.org/abs/2502.00843) <br> [{'name': 'Yuxin Lin, Mengshi Qi, Liang Liu, Huadong Ma'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>Visual Question Answering<br>Vision-Language Models<br>Autonomous Driving | Input: Visual Question Answering task in autonomous driving 视觉问答任务之于自动驾驶<br>Step1: Integrate Vision-Language Models with continual learning 结合视觉语言模型与持续学习<br>Step2: Implement selective memory replay and knowledge distillation 实施选择性记忆重放与知识蒸馏<br>Step3: Apply task-specific projection layer regularization 应用特定任务的投影层正则化<br>Output: Enhanced VQA performance in autonomous driving environments 改进的自动驾驶环境中的视觉问答性能 |
8.5 | [[8.5] 2502.00954 Hypo3D: Exploring Hypothetical Reasoning in 3D](https://arxiv.org/abs/2502.00954) <br> [{'name': 'Ye Mao, Weixun Luo, Junpeng Jing, Anlan Qiu, Krystian Mikolajczyk'}] | 3D Reasoning in Scenes 三维场景推理 | v2<br>3D reasoning<br>visual question answering<br>hypothetical reasoning | Input: Context change descriptions 上下文变化描述<br>Step1: Dataset construction 数据集构建<br>Step2: Model evaluation 模型评估<br>Output: Performance analysis 性能分析 |
8.5 | [[8.5] 2502.00960 SAM-guided Pseudo Label Enhancement for Multi-modal 3D Semantic Segmentation](https://arxiv.org/abs/2502.00960) <br> [{'name': 'Mingyu Yang, Jitong Lu, Hun-Seok Kim'}] | 3D Semantic Segmentation 三维语义分割 | v2<br>3D semantic segmentation<br>domain adaptation<br>pseudo labels<br>autonomous driving | Input: 3D point cloud and SAM masks 3D点云和SAM掩码<br>Step1: Class label determination using majority voting 类别标签确定（使用投票法）<br>Step2: Application of filtering constraints to unreliable labels 对不可靠标签应用过滤约束<br>Step3: Geometry-Aware Progressive Propagation (GAPP) for label propagation 到所有3D点进行标签传播（GAPP方法）<br>Output: Enhanced pseudo-labels and improved segmentation performance 输出：改进的伪标签和增强的分割性能 |
8.5 | [[8.5] 2502.00972 Pushing the Boundaries of State Space Models for Image and Video Generation](https://arxiv.org/abs/2502.00972) <br> [{'name': 'Yicong Hong, Long Mai, Yuan Yao, Feng Liu'}] | Image and Video Generation 图像生成和视频生成 | v2<br>image generation<br>video generation<br>state-space models<br>transformer models | Input: Images and video sequences 图像和视频序列<br>Step1: Develop SSM-Transformer hybrid model 开发SSM-Transformer混合模型<br>Step2: Efficient processing of visual sequences 高效处理视觉序列<br>Step3: Generate images and videos 生成图像和视频<br>Output: High-quality images and dynamic videos 高质量图像和动态视频 |
8.5 | [[8.5] 2502.01004 ZeroBP: Learning Position-Aware Correspondence for Zero-shot 6D Pose Estimation in Bin-Picking](https://arxiv.org/abs/2502.01004) <br> [{'name': 'Jianqiu Chen, Zikun Zhou, Xin Li, Ye Zheng, Tianpeng Bao, Zhenyu He'}] | Autonomous Systems and Robotics 自动驾驶与机器人技术 | v2<br>6D pose estimation<br>bin-picking<br>zero-shot learning<br>robotic manipulation | Input: RGB-D image and CAD model 输入: RGB-D图像和CAD模型<br>Step1: Object detection 物体检测<br>Step2: Point cloud extraction 点云提取<br>Step3: Position-Aware Correspondence learning 位置感知对应学习<br>Step4: Pose estimation 位置估计<br>Output: 6D pose predictions 输出: 6D姿态预测 |
8.5 | [[8.5] 2502.01157 Radiant Foam: Real-Time Differentiable Ray Tracing](https://arxiv.org/abs/2502.01157) <br> [{'name': 'Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi'}] | Neural Rendering 神经渲染 | v2<br>differentiable rendering<br>volumetric meshes<br>real-time rendering | Input: Volumetric mesh representations 体积网格表示<br>Step1: Mesh parameterization 网格参数化<br>Step2: Differentiable ray tracing 可微光线追踪<br>Step3: Rendering and evaluation 渲染与评估<br>Output: Real-time rendering results 实时渲染结果 |
8.5 | [[8.5] 2502.01281 Label Correction for Road Segmentation Using Road-side Cameras](https://arxiv.org/abs/2502.01281) <br> [{'name': 'Henrik Toikka, Eerik Alamikkotervo, Risto Ojala'}] | Autonomous Systems and Robotics 自动驾驶机器人系统 | v2<br>road segmentation<br>autonomous vehicles<br>image registration<br>deep learning | Input: Roadside camera images 道路监控摄像头图像<br>Step1: Automatic data collection 自动数据收集<br>Step2: Semi-automatic annotation method 开发半自动注释方法<br>Step3: Image registration to correct labels 图像配准以修正标签<br>Output: Enhanced road segmentation models 改进的道路分割模型 |
8.5 | [[8.5] 2502.01297 XR-VIO: High-precision Visual Inertial Odometry with Fast Initialization for XR Applications](https://arxiv.org/abs/2502.01297) <br> [{'name': 'Shangjin Zhai, Nan Wang, Xiaomeng Wang, Danpeng Chen, Weijian Xie, Hujun Bao, Guofeng Zhang'}] | Autonomous Systems and Robotics 自动驾驶与机器人技术 | v2<br>Visual Inertial Odometry<br>Initialization<br>Feature Matching<br>AR<br>VR | Input: Visual Inertial Odometry (VIO) data 视觉惯性里程计数据<br>Step1: Initialization using gyroscope and visual measurements 初始化算法<br>Step2: Hybrid feature matching using optical flow and descriptor methods 特征匹配<br>Step3: Evaluation on benchmarks and practical applications 验证和实际应用<br>Output: Enhanced VIO performance 改进的VIO性能 |
8.5 | [[8.5] 2502.01357 Bayesian Approximation-Based Trajectory Prediction and Tracking with 4D Radar](https://arxiv.org/abs/2502.01357) <br> [{'name': 'Dong-In Kim, Dong-Hee Paek, Seung-Hyun Song, Seung-Hyun Kong'}] | Autonomous Driving 自动驾驶 | v2<br>3D multi-object tracking<br>4D Radar | Input: 4D Radar data 4D雷达数据<br>Step1: Object detection using Bayesian approximation 基于贝叶斯近似进行目标检测<br>Step2: Motion prediction with transformer network 使用变换器网络进行运动预测<br>Step3: Two-stage data association integrating Doppler measurements 两阶段数据关联，整合多普勒测量<br>Output: Accurate 3D MOT results 准确的3D多目标跟踪结果 |
8.5 | [[8.5] 2502.01401 Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection](https://arxiv.org/abs/2502.01401) <br> [{'name': 'Boyu Mi, Hanqing Wang, Tai Wang, Yilun Chen, Jiangmiao Pang'}] | 3D Visual Grounding 3D视觉基础 | v2<br>3D visual grounding<br>Large Language Model<br>3D reconstruction<br>vision-language model | Input: Referring utterances and 3D scene scans 参考话语和三维场景扫描<br>Step1: Parse utterance into symbolic expression 将话语解析为符号表达式<br>Step2: Generate spatial relation features 生成空间关系特征<br>Step3: Use VLM to process visual information 使用视觉语言模型处理视觉信息<br>Output: Identified target object 确定目标对象 |
8.0 | [[8.0] 2502.00800 Adversarial Semantic Augmentation for Training Generative Adversarial Networks under Limited Data](https://arxiv.org/abs/2502.00800) <br> [{'name': 'Mengping Yang, Zhe Wang, Ziqiu Chi, Dongdong Li, Wenli Du'}] | Image Generation 图像生成 | v2<br>Generative Adversarial Networks<br>Data Augmentation<br>Image Generation | Input: Limited training data 有限训练数据<br>Step 1: Estimate covariance matrices 估计协方差矩阵<br>Step 2: Identify semantic transformation directions 确定语义转换方向<br>Step 3: Apply adversarial semantic augmentation 应用对抗性语义增强<br>Output: Improved generation quality 改进的生成质量 |
7.5 | [[7.5] 2502.00618 DesCLIP: Robust Continual Adaptation via General Attribute Descriptions for Pretrained Vision-Language Models](https://arxiv.org/abs/2502.00618) <br> [{'name': 'Chiyuan He, Zihuan Qiu, Fanman Meng, Linfeng Xu, Qingbo Wu, Hongliang Li'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>vision-language models<br>knowledge forgetting<br>general attributes | Input: Pretrained Vision-Language Models (VLMs) 预训练视觉语言模型<br>Step1: Generating General Attribute Descriptions 生成通用属性描述<br>Step2: Establishing Vision-GA-Class Associations 建立视觉-通用属性-类关联<br>Step3: Tuning Visual Encoder 调整视觉编码器<br>Output: Enhanced Adaptation with Reduced Knowledge Forgetting 改进的适应性，减少知识遗忘 |
7.5 | [[7.5] 2502.00639 Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer](https://arxiv.org/abs/2502.00639) <br> [{'name': 'Tao Ren, Zishi Zhang, Zehao Li, Jingyang Jiang, Shentao Qin, Guanghao Li, Yan Li, Yi Zheng, Xinping Li, Min Zhan, Yijie Peng'}] | Image Generation 图像生成 | v2<br>Diffusion Model<br>Image Generation<br>Video Generation | Input: Diffusion Model (DM) diffusion模型<br>Step1: Analyze variance and bias variance和偏差分析<br>Step2: Develop Recursive Likelihood Ratio optimizer 开发递归似然比优化器<br>Step3: Validate on image and video tasks 在图像和视频任务上验证<br>Output: Fine-tuned model 改进的模型 |
7.0 | [[7.0] 2502.01530 The in-context inductive biases of vision-language models differ across modalities](https://arxiv.org/abs/2502.01530) <br> [{'name': 'Kelsey Allen, Ishita Dasgupta, Eliza Kosoy, Andrew K. Lampinen'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>vision-language models<br>inductive biases<br>generalization | Input: Visual and textual stimuli 视觉和文本刺激<br>Step1: Inductive bias analysis 偏置分析<br>Step2: Experimental paradigm application 实验范式应用<br>Step3: Data collection and evaluation 数据收集与评估<br>Output: Insights on model generalization 关于模型泛化的见解 |
6.5 | [[6.5] 2502.01524 Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective](https://arxiv.org/abs/2502.01524) <br> [{'name': 'Xiaorui Ma, Haoran Xie, S. Joe Qin'}] | Vision-Language Models (VLMs) 视觉语言模型 | v2<br>multimodal learning<br>Large Language Models<br>parameter-efficient learning<br>Vision-Language Models | Input: Vision-language models 视觉-语言模型<br>Step1: Categorize and review VLLMs 对VLLMs进行分类和审查<br>Step2: Discuss training paradigms 讨论训练范式<br>Step3: Summarize benchmarks 总结基准测试<br>Output: Comprehensive survey report 综合调查报告 |


## Arxiv 2025-02-04

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2502.00173 Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance Segmentation](https://arxiv.org/abs/2502.00173) <br> [{'name': 'Rohan Chacko, Nicolai Haeni, Eldar Khaliullin, Lin Sun, Douglas Lee'}] | 3D Reconstruction and Modeling 三维重建 | 3D instance segmentation 3D实例分割<br>Gaussian Splatted Radiance Fields 高斯喷溅辐射场 | Input: 2D segmentation masks 2D分割掩码<br>Step1: Feature integration 特征集成<br>Step2: 3D Gaussian lifting 3D高斯提升<br>Step3: Segmentation application 分割应用<br>Output: 3D segmented assets 3D分割资产 |
9.5 | [[9.5] 2502.00360 Shape from Semantics: 3D Shape Generation from Multi-View Semantics](https://arxiv.org/abs/2502.00360) <br> [{'name': 'Liangchen Li, Caoliwen Wang, Yuqi Zhou, Bailin Deng, Juyong Zhang'}] | 3D Generation 三维生成 | 3D reconstruction<br>shape generation<br>semantics | Input: Multi-view semantics 多视角语义<br>Step1: Semantic input analysis 语义输入分析<br>Step2: Geometry and appearance distillation from 2D models 从2D模型提取几何与外观<br>Step3: Image restoration and detail enhancement 图像修复与细节增强<br>Step4: Shape reconstruction using neural SDF representation 使用神经签名距离场重建形状<br>Output: Complex detailed 3D meshes 复杂细节的三维网格 |
9.5 | [[9.5] 2502.00801 Environment-Driven Online LiDAR-Camera Extrinsic Calibration](https://arxiv.org/abs/2502.00801) <br> [{'name': 'Zhiwei Huang, Jiaqi Li, Ping Zhong, Rui Fan'}] | 3D Reconstruction and Modeling 三维重建 | LiDAR-camera calibration<br>3D reconstruction<br>data fusion | Input: LiDAR and camera data LiDAR和相机数据<br>Step1: Environmental interpretation 环境解释<br>Step2: Dual-path correspondence matching 双路径对应匹配<br>Step3: Spatial-temporal optimization 空间时间优化<br>Output: Precise extrinsic calibration 精确的外部标定 |
8.5 | [[8.5] 2502.00074 SpikingRTNH: Spiking Neural Network for 4D Radar Object Detection](https://arxiv.org/abs/2502.00074) <br> [{'name': 'Dong-Hee Paek, Seung-Hyun Kong'}] | 3D Object Detection 三维物体检测 | 3D object detection<br>neural networks<br>autonomous driving | Input: 4D Radar data 4D 雷达数据<br>Step1: Process high-density point clouds 处理高密度点云<br>Step2: Implement spiking neural network architecture 实现脉冲神经网络架构<br>Step3: Apply biological top-down inference (BTI) 应用生物学的自上而下推理法<br>Output: Efficient 3D object detection results 高效的三维物体检测结果 |
8.5 | [[8.5] 2502.00262 Your submission contained main.bib and main.tex file, but no main.bbl file (include main.bbl, or submit without main.bib; and remember to verify references)](https://arxiv.org/abs/2502.00262) <br> [{'name': 'Dianwei Chen, Zifan Zhang, Yuchen Liu, Xianfeng Terry Yang'}] | Autonomous Driving 自动驾驶 | hazard detection<br>autonomous driving<br>multimodal data fusion | Input: Multimodal data 输入: 多模态数据<br>Step1: Data integration 数据集成<br>Step2: Hazard detection 危险检测<br>Step3: Spatial localization 空间定位<br>Output: Enhanced hazard prediction 改进的危险预测 |
8.5 | [[8.5] 2502.00315 MonoDINO-DETR: Depth-Enhanced Monocular 3D Object Detection Using a Vision Foundation Model](https://arxiv.org/abs/2502.00315) <br> [{'name': 'Jihyeok Kim, Seongwoo Moon, Sungwon Nah, David Hyunchul Shim'}] | 3D Reconstruction 三维重建 | 3D object detection<br>depth estimation | Input: Monocular images 单目图像<br>Step1: Feature extraction using Vision Transformer 基于视觉变换器的特征提取<br>Step2: Depth estimation using a relative depth model 使用相对深度模型进行深度估计<br>Step3: Object detection using DETR architecture 使用DETR架构进行物体检测<br>Output: Enhanced 3D object detection capabilities 改进的3D物体检测能力 |
8.5 | [[8.5] 2502.00528 Vision-Language Modeling in PET/CT for Visual Grounding of Positive Findings](https://arxiv.org/abs/2502.00528) <br> [{'name': 'Zachary Huemann, Samuel Church, Joshua D. Warner, Daniel Tran, Xin Tie, Alan B McMillan, Junjie Hu, Steve Y. Cho, Meghan Lubner, Tyler J. Bradshaw'}] | VLM & VLA 视觉语言模型 | 3D vision-language model<br>PET/CT<br>visual grounding | Input: PET/CT reports and images PET/CT 报告和图像<br>Step1: Automation of weak labeling pipeline 弱标记生成管道自动化<br>Step2: Data extraction from reports 报告中数据提取<br>Step3: Training of ConTEXTual Net 3D 训练 ConTEXTual Net 3D<br>Output: 3D visual grounding model 3D 视觉定位模型 |
8.5 | [[8.5] 2502.00708 PhiP-G: Physics-Guided Text-to-3D Compositional Scene Generation](https://arxiv.org/abs/2502.00708) <br> [{'name': 'Qixuan Li, Chao Wang, Zongjin He, Yan Peng'}] | 3D Generation 三维生成 | text-to-3D generation<br>compositional scenes<br>physics-guided generation | Input: Complex scene descriptions 复杂场景描述<br>Step1: Scene graph generation 场景图生成<br>Step2: Asset creation using multimodal agents 使用多模态代理进行资产创建<br>Step3: Layout prediction with physical model 使用物理模型进行布局预测<br>Output: Compositional scenes with physical rationality 具有物理合理性的组合场景 |
8.5 | [[8.5] 2502.00843 VLM-Assisted Continual learning for Visual Question Answering in Self-Driving](https://arxiv.org/abs/2502.00843) <br> [{'name': 'Yuxin Lin, Mengshi Qi, Liang Liu, Huadong Ma'}] | VLM & VLA 视觉语言模型与视觉语言对齐 | Vision-Language Models<br>Visual Question Answering<br>autonomous driving<br>continual learning | Input: Visual Question Answering tasks in autonomous driving 在自动驾驶中的视觉问答任务<br>Step1: Integrate Vision-Language Models with continual learning 整合视觉语言模型与持续学习<br>Step2: Implement selective memory replay and knowledge distillation 实施选择性记忆重放和知识蒸馏<br>Step3: Apply task-specific projection layer regularization 应用任务特定投影层正则化<br>Output: Improved VQA system performance 改进的视觉问答系统性能 |
8.5 | [[8.5] 2502.00954 Hypo3D: Exploring Hypothetical Reasoning in 3D](https://arxiv.org/abs/2502.00954) <br> [{'name': 'Ye Mao, Weixun Luo, Junpeng Jing, Anlan Qiu, Krystian Mikolajczyk'}] | 3D Reasoning 3D推理 | 3D reasoning<br>Visual Question Answering<br>scene understanding | Input: Context changes and indoor scene descriptions 上下文变化和室内场景描述<br>Step1: Benchmark formulation 基准测试制定<br>Step2: Model evaluation models performance evaluation 模型性能评估<br>Output: Hypothetical reasoning capabilities 设想推理能力 |
8.5 | [[8.5] 2502.00960 SAM-guided Pseudo Label Enhancement for Multi-modal 3D Semantic Segmentation](https://arxiv.org/abs/2502.00960) <br> [{'name': 'Mingyu Yang, Jitong Lu, Hun-Seok Kim'}] | 3D Reconstruction and Modeling 三维重建 | 3D semantic segmentation<br>domain adaptation<br>pseudo-labels<br>autonomous driving | Input: 3D point cloud and SAM masks 输入: 3D点云和SAM掩码<br>Step1: Class label determination using majority voting 步骤1: 使用投票法确定类别标签<br>Step2: Unreliable mask label filtering using constraints 步骤2: 使用约束过滤不可靠的掩码标签<br>Step3: Geometry-Aware Progressive Propagation (GAPP) to propagate mask labels 步骤3: 使用几何感知逐步传播来传递掩码标签<br>Output: Enhanced pseudo-labels with improved quality 输出: 质量提升的增强伪标签 |
8.5 | [[8.5] 2502.01004 ZeroBP: Learning Position-Aware Correspondence for Zero-shot 6D Pose Estimation in Bin-Picking](https://arxiv.org/abs/2502.01004) <br> [{'name': 'Jianqiu Chen, Zikun Zhou, Xin Li, Ye Zheng, Tianpeng Bao, Zhenyu He'}] | Autonomous Systems and Robotics 自动驾驶 | 6D pose estimation<br>bin-picking<br>robotic manipulation<br>zero-shot learning | Input: Scene instances and CAD models 场景实例与CAD模型<br>Step1: Feature extraction 特征提取<br>Step2: Position-aware correspondence learning 基于位置的对应学习<br>Step3: Pose estimation 位置估计<br>Output: Accurate 6D poses 准确的6D姿势 |
8.5 | [[8.5] 2502.01045 WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human Reconstruction](https://arxiv.org/abs/2502.01045) <br> [{'name': 'Zilong Wang, Zhiyang Dou, Yuan Liu, Cheng Lin, Xiao Dong, Yunhui Guo, Chenxu Zhang, Xin Li, Wenping Wang, Xiaohu Guo'}] | 3D Reconstruction 三维重建 | 3D human reconstruction<br>photorealistic rendering | Input: Monocular video 单目视频<br>Step1: Dual-Space Optimization 双空间优化<br>Step2: Score Distillation Sampling (SDS) 评分蒸馏采样<br>Step3: View Selection_strategy 视图选择策略<br>Step4: Pose Feature Injection 姿态特征注入<br>Output: High-fidelity dynamic human avatars 高保真动态人类虚拟形象 |
8.5 | [[8.5] 2502.01157 Radiant Foam: Real-Time Differentiable Ray Tracing](https://arxiv.org/abs/2502.01157) <br> [{'name': 'Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi'}] | Neural Rendering 神经渲染 | differentiable rendering<br>ray tracing<br>computer vision | Input: Scene representations 场景表示<br>Step1: Implement volumetric mesh ray tracing 实现体积网格光线追踪<br>Step2: Develop a novel scene representation 发展新场景表示<br>Step3: Evaluate rendering speed and quality 评估渲染速度和质量<br>Output: Real-time rendering model 实时渲染模型 |
8.5 | [[8.5] 2502.01281 Label Correction for Road Segmentation Using Road-side Cameras](https://arxiv.org/abs/2502.01281) <br> [{'name': 'Henrik Toikka, Eerik Alamikkotervo, Risto Ojala'}] | Autonomous Driving 自动驾驶 | road segmentation<br>deep learning<br>autonomous vehicles<br>data annotation | Input: Roadside camera feeds 路边摄像头视频<br>Step1: Manual labeling of one frame 手动标注一帧<br>Step2: Transfer labels to other frames 转移标签到其他帧<br>Step3: Compensate for camera movements 使用频域图像配准补偿相机位移<br>Output: Semi-automatically labeled road data 半自动标注的道路数据 |
8.5 | [[8.5] 2502.01297 XR-VIO: High-precision Visual Inertial Odometry with Fast Initialization for XR Applications](https://arxiv.org/abs/2502.01297) <br> [{'name': 'Shangjin Zhai, Nan Wang, Xiaomeng Wang, Danpeng Chen, Weijian Xie, Hujun Bao, Guofeng Zhang'}] | Visual Odometry 视觉里程计 | Visual Inertial Odometry<br>Structure from Motion<br>Augmented Reality<br>Virtual Reality | Input: Visual inertial measurements 视觉惯性测量<br>Step1: Robust initialization initialization 稳健初始化<br>Step2: Feature matching 特征匹配<br>Step3: State estimation 状态估计<br>Output: Accurate visual inertial odometry result 精确的视觉惯性里程计结果 |
8.5 | [[8.5] 2502.01356 Quasi-Conformal Convolution : A Learnable Convolution for Deep Learning on Riemann Surfaces](https://arxiv.org/abs/2502.01356) <br> [{'name': 'Han Zhang, Tsz Lok Ip, Lok Ming Lui'}] | 3D Reconstruction and Modeling 3D重建 | 3D facial analysis<br>Riemann surfaces | Input: Geometric data and Riemann surfaces 几何数据和黎曼曲面<br>Step1: Define quasi-conformal mappings 定义准保形映射<br>Step2: Develop Quasi-Conformal Convolution operators 开发准保形卷积算子<br>Step3: Implement Quasi-Conformal Convolutional Neural Network (QCCNN) 实现准保形卷积神经网络<br>Output: Adaptive convolution for geometric data 自适应卷积用于几何数据 |
8.5 | [[8.5] 2502.01357 Bayesian Approximation-Based Trajectory Prediction and Tracking with 4D Radar](https://arxiv.org/abs/2502.01357) <br> [{'name': 'Dong-In Kim, Dong-Hee Paek, Seung-Hyun Song, Seung-Hyun Kong'}] | Robotic Perception 机器人感知 | 3D multi-object tracking<br>Bayesian approximation<br>autonomous driving | Input: 4D Radar data 4D 雷达数据<br>Step1: Motion prediction using transformer-based network 使用基于变换器的网络进行运动预测<br>Step2: Bayesian approximation for detection and prediction 步骤 2: 检测和预测中的贝叶斯近似<br>Step3: Two-stage data association leveraging Doppler measurements 基于多普勒测量的两阶段数据关联<br>Output: Enhanced multi-object tracking performance 提升的多目标跟踪性能 |
8.5 | [[8.5] 2502.01401 Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection](https://arxiv.org/abs/2502.01401) <br> [{'name': 'Boyu Mi, Hanqing Wang, Tai Wang, Yilun Chen, Jiangmiao Pang'}] | 3D Visual Grounding 3D视觉定位 | 3D visual grounding<br>weakly supervised learning | Input: 3D visual information and language 3D视觉信息与语言<br>Step1: Code generation using LLM 通过LLM生成代码<br>Step2: Spatial relationship computation 空间关系计算<br>Step3: Quality evaluation and optimization 质量评估和优化<br>Output: Efficient grounding results 高效的定位结果 |
8.5 | [[8.5] 2502.01405 FourieRF: Few-Shot NeRFs via Progressive Fourier Frequency Control](https://arxiv.org/abs/2502.01405) <br> [{'name': 'Diego Gomez, Bingchen Gong, Maks Ovsjanikov'}] | 3D Reconstruction 三维重建 | Few-Shot NeRFs 少样本神经辐射场<br>3D Reconstruction 三维重建 | Input: Scene images 场景图像<br>Step1: Curriculum training curriculum training 课程训练<br>Step2: Feature parameterization 特征参数化<br>Step3: Scene complexity increment 增加场景复杂性<br>Output: High-quality reconstruction 高质量重建 |
8.0 | [[8.0] 2502.00342 Embodied Intelligence for 3D Understanding: A Survey on 3D Scene Question Answering](https://arxiv.org/abs/2502.00342) <br> [{'name': 'Zechuan Li, Hongshan Yu, Yihao Ding, Yan Li, Yong He, Naveed Akhtar'}] | 3D Reconstruction and Modeling 3D重建与建模 | 3D scene question answering<br>multimodal modelling<br>datasets | Input: 3D scene data 3D场景数据<br>Step1: Systematic review of datasets 数据集的系统评审<br>Step2: Analysis of methodologies 方法论分析<br>Step3: Evaluation of metrics 评估指标<br>Output: Comprehensive understanding of 3D SQA 3D场景问答的综合理解 |
8.0 | [[8.0] 2502.00800 Adversarial Semantic Augmentation for Training Generative Adversarial Networks under Limited Data](https://arxiv.org/abs/2502.00800) <br> [{'name': 'Mengping Yang, Zhe Wang, Ziqiu Chi, Dongdong Li, Wenli Du'}] | Image Generation 图像生成 | Generative Adversarial Networks<br>data augmentation<br>image synthesis<br>semantic features | Input: Limited image datasets 有限图像数据集<br>Step1: Estimate covariance matrices 估计协方差矩阵<br>Step2: Identify meaningful transformation directions 识别有意义的转化方向<br>Step3: Apply transformations to semantic features 对语义特征应用转化<br>Output: Enhanced synthetic images 增强合成图像 |
7.5 | [[7.5] 2502.00333 BiMaCoSR: Binary One-Step Diffusion Model Leveraging Flexible Matrix Compression for Real Super-Resolution](https://arxiv.org/abs/2502.00333) <br> [{'name': 'Kai Liu, Kaicheng Yang, Zheng Chen, Zhiteng Li, Yong Guo, Wenbo Li, Linghe Kong, Yulun Zhang'}] | Image Generation 图像生成 | super-resolution<br>diffusion model<br>binarization<br>model compression | Input: Diffusion model for super-resolution 超分辨率扩散模型<br>Step1: Binarization of model models 模型的二值化<br>Step2: One-step distillation into extreme compression 一步蒸馏以实现极端压缩<br>Step3: Integration of sparse and low rank matrix branches 结合稀疏和低秩矩阵分支<br>Output: Compressed and accelerated super-resolution model 压缩和加速的超分辨率模型 |
7.5 | [[7.5] 2502.00500 Video Latent Flow Matching: Optimal Polynomial Projections for Video Interpolation and Extrapolation](https://arxiv.org/abs/2502.00500) <br> [{'name': 'Yang Cao, Zhao Song, Chiwun Yang'}] | Image and Video Generation 图像生成 | video generation<br>interpolation<br>extrapolation | Input: Video frames 视频帧<br>Step1: Hypothesis generation 假设生成<br>Step2: Optimal projection approximation 最优投影近似<br>Step3: Interpolation and extrapolation 插值和外推<br>Output: Time-dependent video frames 时间依赖视频帧 |
7.5 | [[7.5] 2502.00639 Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer](https://arxiv.org/abs/2502.00639) <br> [{'name': 'Tao Ren, Zishi Zhang, Zehao Li, Jingyang Jiang, Shentao Qin, Guanghao Li, Yan Li, Yi Zheng, Xinping Li, Min Zhan, Yijie Peng'}] | Image Generation 图像生成 | Diffusion Model<br>image generation<br>video generation | Input: Probabilistic diffusion model 概率扩散模型<br>Step1: Pre-training on unlabeled data 在无标签数据上进行预训练<br>Step2: Recursive Likelihood Ratio optimizer proposal 提出递归似然比优化器<br>Step3: Implementation of zero-order gradient estimation 零阶梯度估计的实施<br>Output: Aligned diffusion models 对齐的扩散模型 |
7.5 | [[7.5] 2502.00662 Mitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with Multi-modal Prototypes and Image Bias Estimation](https://arxiv.org/abs/2502.00662) <br> [{'name': 'Yimu Wang, Evelien Riddell, Adrian Chow, Sean Sedwards, Krzysztof Czarnecki'}] | VLM & VLA 视觉语言模型与对齐 | vision-language models<br>out-of-distribution detection<br>few-shot learning | Input: ID image and text prototypes 输入: ID图像和文本原型<br>Step1: Theoretical analysis 理论分析<br>Step2: Incorporation of image prototypes 图像原型的整合<br>Step3: Development of biased prompts generation (BPG) module 偏差提示生成(BPG)模块的开发<br>Step4: Implementation of image-text consistency (ITC) module 图像文本一致性(ITC)模块的实施<br>Output: Enhanced VLM-based OOD detection performance 输出: 改进的基于VLM的OOD检测性能 |
7.5 | [[7.5] 2502.00711 VIKSER: Visual Knowledge-Driven Self-Reinforcing Reasoning Framework](https://arxiv.org/abs/2502.00711) <br> [{'name': 'Chunbai Zhang, Chao Wang, Yang Zhou, Yan Peng'}] | Vision-Language Models (VLMs) 视觉语言模型 | visual reasoning<br>evidence-based reasoning<br>VLM | Input: Visual information (images/videos) 输入: 视觉信息（图像/视频）<br>Step1: Extract fine-grained visual knowledge from visual relationships 第一步: 从视觉关系中提取细粒度视觉知识<br>Step2: Paraphrase questions with underspecification using extracted knowledge 第二步: 利用提取的知识对欠规范的问题进行改写<br>Step3: Employ Chain-of-Evidence prompting for interpretable reasoning 第三步: 使用证据链提示进行可解释推理<br>Output: Enhanced visual reasoning capabilities 输出: 改进的视觉推理能力 |
7.5 | [[7.5] 2502.00719 Vision and Language Reference Prompt into SAM for Few-shot Segmentation](https://arxiv.org/abs/2502.00719) <br> [{'name': 'Kosuke Sakurai, Ryotaro Shimizu, Masayuki Goto'}] | VLM & VLA 视觉语言模型与对齐 | few-shot segmentation<br>vision-language model | Input: Annotated reference images and text labels 参考图像和文本标签<br>Step1: Input visual and semantic reference信息输入视觉和语义参考<br>Step2: Integrate prompt embeddings into SAM 将提示嵌入集成到SAM<br>Step3: Few-shot segmentation via VLP-SAM 通过VLP-SAM进行少样本分割<br>Output: High-performance segmentation results 高性能的分割结果 |
7.5 | [[7.5] 2502.00972 Pushing the Boundaries of State Space Models for Image and Video Generation](https://arxiv.org/abs/2502.00972) <br> [{'name': 'Yicong Hong, Long Mai, Yuan Yao, Feng Liu'}] | Image Generation 图像生成 | image generation<br>video generation | Input: Visual sequences 视觉序列<br>Step1: Model development 模型开发<br>Step2: Integration of SSM and Transformers SSM与变换器的整合<br>Step3: Evaluation of generated outputs 生成结果的评估<br>Output: Generated images and videos 生成的图像和视频 |
7.5 | [[7.5] 2502.01524 Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective](https://arxiv.org/abs/2502.01524) <br> [{'name': 'Xiaorui Ma, Haoran Xie, S. Joe Qin'}] | VLM & VLA 视觉语言模型与对齐 | Vision-Language<br>Large Language Models<br>parameter efficiency | Step1: Introduce architecture of LLMs 介绍LLM架构<br>Step2: Discuss parameter-efficient learning methods 讨论参数效率学习方法<br>Step3: Present taxonomy of modality integrators 提出模态集成器分类<br>Step4: Review training paradigms and efficiency considerations 回顾训练范式及效率考虑<br>Step5: Compare experimental results of representative models 比较代表模型的实验结果 |
7.5 | [[7.5] 2502.01530 The in-context inductive biases of vision-language models differ across modalities](https://arxiv.org/abs/2502.01530) <br> [{'name': 'Kelsey Allen, Ishita Dasgupta, Eliza Kosoy, Andrew K. Lampinen'}] | Vision-Language Models (VLMs) 视觉语言模型 | vision-language models<br>inductive biases<br>generalization | Input: Stimuli presented in vision and text 视觉和文本中呈现的刺激<br>Step1: Conduct experiments 进行实验<br>Step2: Analyze generalization across models 分析模型间的概括性<br>Output: Insights on inductive biases regarding shape and color 对形状和颜色的归纳偏见的见解 |
5.0 | [[5.0] 2502.00618 DesCLIP: Robust Continual Adaptation via General Attribute Descriptions for Pretrained Vision-Language Models](https://arxiv.org/abs/2502.00618) <br> [{'name': 'Chiyuan He, Zihuan Qiu, Fanman Meng, Linfeng Xu, Qingbo Wu, Hongliang Li'}] | Vision-Language Models (VLMs) 视觉语言模型 | vision-language models<br>continual adaptation<br>attribute descriptions | Input: Visual features and class text visuals 视觉特征和类别文本<br>Step1: Generate general attribute descriptions 生成一般属性描述<br>Step2: Design anchor-based embedding filter 设计基于锚点的嵌入过滤器<br>Step3: Tune visual encoder 调整视觉编码器<br>Output: Robust vision-GA-class associations 稳健的视觉-一般属性-类别关联 |


## Arxiv 2025-01-31

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2501.17978v2 VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting](http://arxiv.org/abs/2501.17978v2) | 3D generation 3D生成 | 3D Gaussian Splatting<br>view-dependent representation<br>3D高斯渲染<br>视角依赖表示 | input: images 图片<br>extend the 3D Gaussian Splatting model 扩展3D高斯渲染模型<br>introduce an additional symmetric matrix 引入额外的对称矩阵<br>achieve view-dependent opacity representation 实现视角依赖的透明度表示<br>output: improved 3D scene reconstruction 输出：改进的3D场景重建 |
8.5 | [[8.5] 2501.19319v1 Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven Surface Normal-aware Tracking and Mapping](http://arxiv.org/abs/2501.19319v1) | 3D reconstruction 三维重建 | 3D reconstruction<br>3D Gaussian Splatting<br>endoscopic SLAM<br>depth reconstruction<br>三维重建<br>3D高斯斑点<br>内窥镜SLAM<br>深度重建 | input: endoscopic image sequences 内窥镜图像序列<br>Step 1: tracking using Gaussian Splatting 使用高斯斑点的跟踪<br>Step 2: mapping and bundle adjustment 映射与束调整<br>Step 3: surface normal-aware reconstruction 结合表面法向量进行重构<br>output: accurate 3D reconstruction and real-time tracking 输出: 精确的3D重建与实时跟踪 |
8.5 | [[8.5] 2501.19270v1 Imagine with the Teacher: Complete Shape in a Multi-View Distillation Way](http://arxiv.org/abs/2501.19270v1) | 3D reconstruction  三维重建 | Point Cloud Completion<br>3D Shape Completion<br>Knowledge Distillation<br>Points Completion<br>点云补全<br>3D形状补全<br>知识蒸馏<br>点补全 | input: incomplete point cloud  有缺失的点云<br>step1: apply autoencoder to encode the point cloud  应用自编码器对点云进行编码<br>step2: use knowledge distillation for completion  使用知识蒸馏进行补全<br>step3: output: completed 3D shape  输出：完整的3D形状 |
8.5 | [[8.5] 2501.19196v1 RaySplats: Ray Tracing based Gaussian Splatting](http://arxiv.org/abs/2501.19196v1) | 3D generation 3D生成 | 3D Gaussian Splatting<br>Gaussian Splatting<br>3D高斯喷溅<br>高斯喷溅 | Input: 2D images 2D图像<br>Ray-tracing mechanism 射线追踪机制<br>Intersection computation 交点计算<br>Ray-tracing algorithms construction 射线追踪算法构建<br>Final 3D object with lighting and shadows 最终带有光影效果的三维物体 |
8.5 | [[8.5] 2501.19088v1 JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting](http://arxiv.org/abs/2501.19088v1) | 3D generation 3D生成 | 3D Gaussian Splatting<br>3D reconstruction<br>实时渲染<br>3D高斯分喷<br>三维重建 | input: 3D key points (输入：3D关键点)<br>Step 1: Create a joint-driven 3D Gaussian representation (步骤1：创建联合驱动的3D高斯表示)<br>Step 2: Implement differentiable spatial transformations (步骤2：实现可微分的空间变换)<br>Step 3: Apply real-time shadow simulation method (步骤3：应用实时阴影模拟方法)<br>output: High-fidelity hand images (输出：高保真的手部图像) |
8.5 | [[8.5] 2501.18982v1 OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation](http://arxiv.org/abs/2501.18982v1) | 3D generation 3D生成 | 3D generation<br>3D gaussian<br>物体生成<br>3D高斯 | input: 3D assets 3D资产<br>extract: physical properties 提取物理属性<br>generate: physics-based dynamics 生成基于物理的动态<br>output: dynamic scene 输出动态场景 |
7.5 | [[7.5] 2501.19382v1 LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention Networks](http://arxiv.org/abs/2501.19382v1) | Autonomous Driving 自动驾驶 | LiDAR<br>loop closure detection<br>graph attention networks<br>place recognition<br>semanitic registration<br>激光雷达<br>回环闭合检测<br>图注意力网络<br>地点识别<br>语义注册 | input: semantic graphs 语义图<br>step1: encode semantic graphs using graph attention networks 使用图注意力网络编码语义图<br>step2: compare graph vectors to identify loop closure 比较图向量以识别回环闭合<br>step3: estimate 6 DoF pose constraint using semantic registration 使用语义注册估计6自由度位姿约束<br>output: loop closure detection results 回环闭合检测结果 |
7.5 | [[7.5] 2501.19259v1 Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for Autonomous Drone FlighT at the Edge](http://arxiv.org/abs/2501.19259v1) | Autonomous Driving 自主驾驶 | Autonomous Driving<br>Neuromorphic Vision<br>Real-time Navigation<br>Autonomous Systems<br>自驾驶<br>神经形态视觉<br>实时导航<br>自主系统 | Input: Human speech commands 人类语音指令<br>Step 1: Translate speech into planning commands 将语音翻译成规划指令<br>Step 2: Execute commands using neuromorphic vision 执行命令使用神经形态视觉<br>Step 3: Navigate and avoid obstacles in real-time 实时导航和避免障碍<br>Output: Autonomous drone navigation output 自主无人机导航输出 |
7.5 | [[7.5] 2501.19252v1 Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search](http://arxiv.org/abs/2501.19252v1) | Video Generation 视频生成 | video generation<br>text-to-video models<br>视频生成<br>文本到视频模型 | input: diffusion model inputs 输入：扩散模型输入<br>step1: align video frames with text prompts 步骤1：将视频帧与文本提示对齐<br>step2: utilize a beam search strategy to optimize output 使用束搜索策略优化输出<br>step3: compute metrics for perceptual quality evaluation 计算感知质量评估的指标<br>output: high-quality, aligned video generation 输出：高质量、对齐的视频生成 |
7.5 | [[7.5] 2501.19035v1 SynthmanticLiDAR: A Synthetic Dataset for Semantic Segmentation on LiDAR Imaging](http://arxiv.org/abs/2501.19035v1) | Autonomous Driving 自动驾驶 | Semantic Segmentation<br>LiDAR Imaging<br>Autonomous Driving<br>合成分割<br>LiDAR成像<br>自动驾驶 | input: LiDAR data 输入: LiDAR 数据<br>step1: generate synthetic dataset 生成合成数据集<br>step2: utilize CARLA simulator 使用 CARLA 模拟器<br>step3: train segmentation algorithms 训练分割算法<br>output: improved segmentation performance 输出: 改进的分割性能 |
7.5 | [[7.5] 2501.17159v2 IC-Portrait: In-Context Matching for View-Consistent Personalized Portrait](http://arxiv.org/abs/2501.17159v2) | Image Generation 图像生成 | personalized portrait generation<br>identity preservation<br>view-consistent reconstruction<br>个性化肖像生成<br>身份保留<br>视角一致重建 | input: reference images 参考图像<br>step1: Lighting-Aware Stitching 光照感知拼接<br>step2: View-Consistent Adaptation 视角一致自适应<br>step3: ControlNet-like supervision 控制网络样监督<br>output: personalized portraits 个性化肖像 |
6.5 | [[6.5] 2501.18994v1 VKFPos: A Learning-Based Monocular Positioning with Variational Bayesian Extended Kalman Filter Integration](http://arxiv.org/abs/2501.18994v1) | Autonomous Driving (自动驾驶) | Monocular Positioning<br>Extended Kalman Filter<br>Deep Learning<br>Single-shot<br>单目定位<br>扩展卡尔曼滤波<br>深度学习<br>单次 | input: monocular images 单目图像<br>step1: Absolute Pose Regression (APR) 绝对姿态回归<br>step2: Relative Pose Regression (RPR) 相对姿态回归<br>step3: Integrate APR and RPR using EKF 通过扩展卡尔曼滤波整合APR和RPR<br>output: accurate positioning results 精确定位结果 |
6.0 | [[6.0] 2501.19331v1 Consistent Video Colorization via Palette Guidance](http://arxiv.org/abs/2501.19331v1) | Video Generation 视频生成 | Video Colorization<br>Stable Video Diffusion<br>Palette Guidance<br>视频上色<br>稳定视频扩散<br>调色板引导 | input: video sequences 视频序列<br>step 1: design palette-based color guider 设计调色板引导器<br>step 2: utilize Stable Video Diffusion as base model 利用稳定视频扩散作为基础模型<br>step 3: generate vivid colors using color context 根据颜色上下文生成生动的颜色<br>output: colorized video sequences 上色的视频序列 |
5.5 | [[5.5] 2501.18865v1 REG: Rectified Gradient Guidance for Conditional Diffusion Models](http://arxiv.org/abs/2501.18865v1) | Image Generation 图像生成 | conditional generation<br>diffusion models<br>conditional generation 条件生成<br>扩散模型 | input: guidance techniques 指导技术<br>step1: replace the scaled marginal distribution target 替换缩放的边际分布目标<br>step2: implement rectified gradient guidance 实施矩形梯度指导<br>step3: conduct experiments on image generation tasks 进行图像生成任务的实验<br>output: improved image generation results 改进的图像生成结果 |


## Arxiv 2025-01-31

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
9.5 | [[9.5] 2501.19196v1 RaySplats: Ray Tracing based Gaussian Splatting](http://arxiv.org/abs/2501.19196v1) | 3D generation 三维生成 | 3D Gaussian Splatting<br>Ray Tracing<br>3D高斯点云<br>光线追踪 | input: 2D images 2D图像<br>process: Gaussian Splatting 高斯点云渲染<br>process: ray tracing based on Gaussian primitives 基于高斯原始体的光线追踪<br>output: 3D objects with light and shadow effects 输出具有光影效果的3D物体 |
9.0 | [[9.0] 2501.17978v2 VoD-3DGS: View-opacity-Dependent 3D Gaussian Splatting](http://arxiv.org/abs/2501.17978v2) | 3D generation 3D生成 | 3D Gaussian Splatting<br>view-dependent rendering<br>3D高斯点云<br>视角依赖的渲染 | input: 3D scene reconstruction from images 3D场景重建从图像中提取<br>step 1: extend 3D Gaussian Splatting model 扩展3D高斯点云模型<br>step 2: introduce symmetric matrix to enhance opacity representation 引入对称矩阵以增强不透明性表示<br>step 3: optimize suppression of Gaussians based on viewer perspective 根据观察者视角优化高斯的抑制<br>output: improved representation of view-dependent reflections and specular highlights 输出：改进视角依赖的反射和镜面高光的表示 |
8.5 | [[8.5] 2501.19319v1 Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven Surface Normal-aware Tracking and Mapping](http://arxiv.org/abs/2501.19319v1) | 3D reconstruction  三维重建 | 3D Gaussian Splatting<br>SLAM<br>endoscopic reconstruction<br>depth reconstruction<br>3D 高斯点<br>SLAM<br>内窥镜重建<br>深度重建 | input: endoscopic images 内窥镜图像<br>step1: surface normal-aware tracking 表面法线感知跟踪<br>step2: accurate mapping 精确地图构建<br>step3: bundle adjustment 捆绑调整<br>output: geometrically accurate 3D reconstruction 准确的三维重建 |
8.5 | [[8.5] 2501.19252v1 Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search](http://arxiv.org/abs/2501.19252v1) | Video Generation 视频生成 | Text-to-video<br>Diffusion models<br>Video generation<br>评分调整<br>文本转视频<br>扩散模型<br>视频生成<br>奖励校准 | input: video generation prompts 视频生成提示<br>step1: employ diffusion latent beam search 使用扩散潜在光束搜索<br>step2: maximize alignment reward 最大化对齐奖励<br>step3: improve perceptual quality 提升感知质量<br>output: high-quality video optimized for natural movement 输出：高质量视频，优化自然运动 |
8.5 | [[8.5] 2501.19088v1 JGHand: Joint-Driven Animatable Hand Avater via 3D Gaussian Splatting](http://arxiv.org/abs/2501.19088v1) | 3D generation 3D生成 | 3D Gaussian Splatting<br>animatable hand avatar<br>3D高斯喷涂<br>可动画手部化身 | input: 3D key points 3D关键点<br>Jointly 3D Gaussian Splatting (3DGS) joint-driven representation 联合3D高斯喷涂（3DGS）驱动表示<br>apply spatial transformations based on 3D key points 基于3D关键点应用空间变换<br>real-time rendering and shadow simulation 实时渲染和阴影模拟<br>output: animatable high-fidelity hand images 输出：可动画的高保真手部图像 |
8.5 | [[8.5] 2501.18982v1 OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation](http://arxiv.org/abs/2501.18982v1) | 3D generation 3D生成 | 3D generation<br>3D gaussian<br>3D生成<br>3D高斯 | input: user-specified prompts 用户指定的提示<br>step1: define a scene according to user prompts 根据用户提示定义场景<br>step2: estimate material weighting factors using a pretrained video diffusion model 使用预训练的视频扩散模型估计材料权重因子<br>step3: represent each 3D asset as a collection of constitutive 3D Gaussians 将每个3D资产表示为一组组成的3D高斯分布<br>output: a physics-based 3D dynamic scene 输出：基于物理的3D动态场景 |
8.0 | [[8.0] 2501.19270v1 Imagine with the Teacher: Complete Shape in a Multi-View Distillation Way](http://arxiv.org/abs/2501.19270v1) | 3D reconstruction三维重建 | Point Cloud Completion<br>Multi-view Distillation<br>3D Shape Recovery<br>点云补全<br>多视图蒸馏<br>3D形状恢复 | input: incomplete point cloud 输入: 不完整的点云<br>step1: apply autoencoder architecture 应用自编码器架构<br>step2: use knowledge distillation strategy to enhance completion 使用知识蒸馏策略以增强完成度<br>step3: output: completed point cloud 输出: 完整的点云 |
7.5 | [[7.5] 2501.19382v1 LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention Networks](http://arxiv.org/abs/2501.19382v1) | Autonomous Driving 自主驾驶 | Loop Closure Detection<br>Semantic Graphs<br>Graph Attention Networks<br>闭环检测<br>语义图<br>图注意力网络 | input: point cloud 输入: 点云<br>step1: encode semantic graphs using graph attention networks 步骤1: 使用图注意力网络编码语义图<br>step2: generate graph vectors through self-attention mechanisms 步骤2: 通过自注意力机制生成图向量<br>step3: compare graph vectors to detect loop closure 步骤3: 比较图向量以检测闭环<br>output: loop closure candidates 输出: 闭环候选 |
7.5 | [[7.5] 2501.19035v1 SynthmanticLiDAR: A Synthetic Dataset for Semantic Segmentation on LiDAR Imaging](http://arxiv.org/abs/2501.19035v1) | Autonomous Driving 自主驾驶 | Semantic segmentation<br>LiDAR imaging<br>autonomous driving<br>合成分割<br>LiDAR成像<br>自主驾驶 | input: LiDAR images (输入: LiDAR图像)<br>modify CARLA simulator (修改CARLA模拟器)<br>generate SynthmanticLiDAR dataset (生成SynthmanticLiDAR数据集)<br>evaluate with transfer learning (使用迁移学习进行评估)<br>output: improved semantic segmentation performance (输出: 改进的语义分割性能) |
7.5 | [[7.5] 2501.17159v2 IC-Portrait: In-Context Matching for View-Consistent Personalized Portrait](http://arxiv.org/abs/2501.17159v2) | Image Generation 图像生成 | Personalized Portrait Generation<br>3D-aware relighting<br>个性化肖像生成<br>具3D感知的重光照 | Input: reference portrait images 参考肖像图像<br>Step 1: Lighting-Aware Stitching 具光照感知的拼接<br>Step 2: View-Consistent Adaptation 具视图一致的适配<br>Output: personalized portraits with identity preservation 具有身份保留的个性化肖像 |
7.0 | [[7.0] 2501.19243v1 Accelerating Diffusion Transformer via Error-Optimized Cache](http://arxiv.org/abs/2501.19243v1) | Image Generation 图像生成 | Image Generation<br>Diffusion Transformer<br>ImageNet Dataset<br>图像生成<br>扩散变换器<br>ImageNet数据集 | input: Diffusion Transformer features (扩散变换器特征)<br>extract caching differences (提取缓存差异)<br>optimize cache based on errors (基于错误优化缓存)<br>output: improved generated images (输出: 改进的生成图像) |
6.5 | [[6.5] 2501.19259v1 Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for Autonomous Drone FlighT at the Edge](http://arxiv.org/abs/2501.19259v1) | Autonomous Driving 自主驾驶 | autonomous driving<br>natural language processing<br>neuroscience<br>autonomous navigation<br>自主驾驶<br>自然语言处理<br>神经科学<br>自主导航 | input: human speech and dynamic environment 输入：人类语言和动态环境<br>step1: translate human speech into planning commands 步骤1：将人类语言翻译为规划命令<br>step2: navigate and avoid obstacles using neuromorphic vision 步骤2：利用神经形态视觉导航并避免障碍物<br>output: real-time autonomous navigation output 实时自主导航结果 |
6.5 | [[6.5] 2501.18994v1 VKFPos: A Learning-Based Monocular Positioning with Variational Bayesian Extended Kalman Filter Integration](http://arxiv.org/abs/2501.18994v1) | Autonomous Driving 自主驾驶 | monocular positioning<br>extended kalman filter<br>variational bayesian inference<br>单目定位<br>扩展卡尔曼滤波<br>变分贝叶斯推理 | input: monocular images 单目图像<br>step1: Absolute Pose Regression (APR) 绝对姿态回归<br>step2: Relative Pose Regression (RPR) 相对姿态回归<br>step3: Integration with Extended Kalman Filter (EKF) 通过扩展卡尔曼滤波整合<br>output: accurate positional predictions 准确的位置信息预测 |


## Arxiv 2025-01-30

Relavance | Title | Research Topic | Keywords | Pipeline
|------|---------------|----------------|----------|---------|
8.5 | [[8.5] 2501.18594v1 Foundational Models for 3D Point Clouds: A Survey and Outlook](http://arxiv.org/abs/2501.18594v1) | 3D reconstruction 3D重建 | 3D point clouds<br>foundational models<br>3D视觉理解<br>基础模型<br>3D点云 | input: 3D point clouds 3D点云<br>step1: review of foundational models FMs 基础模型的回顾<br>step2: categorize use of FMs in 3D tasks 分类基础模型在3D任务中的应用<br>step3: summarize state-of-the-art methods 总结最新的方法<br>output: comprehensive overview of FMs for 3D understanding 输出：基础模型在3D理解中的综合概述 |
8.5 | [[8.5] 2501.18162v1 IROAM: Improving Roadside Monocular 3D Object Detection Learning from Autonomous Vehicle Data Domain](http://arxiv.org/abs/2501.18162v1) | Autonomous Driving 自动驾驶 | 3D object detection<br>autonomous driving<br>3D对象检测<br>自动驾驶 | input: roadside data and vehicle-side data<br>In-Domain Query Interaction module learns content and depth information<br>Cross-Domain Query Enhancement decouples queries into semantic and geometry parts<br>outputs enhanced object queries |
8.5 | [[8.5] 2501.18110v1 Lifelong 3D Mapping Framework for Hand-held & Robot-mounted LiDAR Mapping Systems](http://arxiv.org/abs/2501.18110v1) | 3D reconstruction 三维重建 | 3D Mapping<br>3D Reconstruction<br>Lifelong Mapping<br>激光雷达<br>三维映射<br>三维重建<br>终身映射 | Input: Hand-held and robot-mounted LiDAR maps 输入：手持和机器人安装的激光雷达地图<br>Dynamic point removal algorithm 动态点去除算法<br>Multi-session map alignment using feature descriptor matching and fine registration 多会话地图对齐，使用特征描述符匹配和精细配准<br>Map change detection to identify changes between aligned maps 地图变化检测以识别对齐地图之间的变化<br>Map version control for maintaining current environmental state and querying changes 地图版本控制，用于维护当前环境状态和查询变化 |
8.0 | [[8.0] 2501.18595v1 ROSA: Reconstructing Object Shape and Appearance Textures by Adaptive Detail Transfer](http://arxiv.org/abs/2501.18595v1) | Mesh Reconstruction 网格重建 | Mesh Reconstruction<br>3D reconstruction<br>网格重建<br>三维重建 | input: limited set of images 限制的图像集<br>step1: optimize mesh geometry 优化网格几何形状<br>step2: refine mesh with spatially adaptive resolution 使用空间自适应分辨率细化网格<br>step3: reconstruct high-resolution textures 重新构建高分辨率纹理<br>output: textured mesh with detailed appearance 带有详细外观的纹理网格 |
7.5 | [[7.5] 2501.18590v1 DiffusionRenderer: Neural Inverse and Forward Rendering with Video Diffusion Models](http://arxiv.org/abs/2501.18590v1) | Rendering Techniques 渲染技术 | Inverse Rendering<br>Forward Rendering<br>Video Diffusion Models<br>Inverse渲染<br>正向渲染<br>视频扩散模型 | input: real-world videos, 真实世界视频<br>step1: estimate G-buffers using inverse rendering model, 使用逆向渲染模型估计G-buffer<br>step2: generate photorealistic images from G-buffers, 从G-buffer生成照片级真实图像<br>output: relit images, material edited images, realistic object insertions, 重新照明图像，材料编辑图像，逼真的物体插入 |
7.5 | [[7.5] 2501.18315v1 Surface Defect Identification using Bayesian Filtering on a 3D Mesh](http://arxiv.org/abs/2501.18315v1) | Mesh Reconstruction 网格重建 | 3D Mesh<br>Mesh Reconstruction<br>3D网格<br>网格重建 | input: CAD model and point cloud data 输入：CAD模型和点云数据<br>transform CAD model into polygonal mesh 将CAD模型转换为多边形网格<br>apply weighted least squares algorithm 应用加权最小二乘算法<br>estimate state based on point cloud measurements 根据点云测量估计状态<br>output: high-precision defect identification 输出：高精度缺陷识别 |
7.5 | [[7.5] 2501.17636v2 Efficient Interactive 3D Multi-Object Removal](http://arxiv.org/abs/2501.17636v2) | 3D reconstruction 三维重建 | 3D scene understanding<br>multi-object removal<br>3D场景理解<br>多对象移除 | input: selected areas and objects for removal 选定的移除区域和对象<br>step1: mask matching and refinement mask 匹配和细化掩码步骤<br>step2: homography-based warping 同伦变换基础的扭曲<br>step3: inpainting process 修复过程<br>output: modified 3D scene 修改后的3D场景 |
7.0 | [[7.0] 2501.18246v1 Ground Awareness in Deep Learning for Large Outdoor Point Cloud Segmentation](http://arxiv.org/abs/2501.18246v1) | 3D reconstruction  三维重建 | point cloud segmentation<br>outdoor point clouds<br>semantic segmentation<br>point cloud<br>关键点云分割<br>户外点云<br>语义分割<br>点云 | input: outdoor point clouds 户外点云<br>compute Digital Terrain Models (DTMs) 计算数字地形模型<br>employ RandLA-Net for segmentation 使用 RandLA-Net 进行分割<br>evaluate performance on datasets 评估在数据集上的表现<br>integrate relative elevation features 集成相对高程特征 |
6.5 | [[6.5] 2501.18494v1 Runway vs. Taxiway: Challenges in Automated Line Identification and Notation Approaches](http://arxiv.org/abs/2501.18494v1) | Autonomous Driving 自动驾驶 | Automated line identification 自动化线识别<br>Convolutional Neural Network 卷积神经网络<br>runway markings 跑道标记<br>autonomous systems 自动化系统<br>labeling algorithms 标记算法 | input: runway and taxiway images 跑道和滑行道图像<br>Step 1: color threshold adjustment 颜色阈值调整<br>Step 2: refine region of interest selection 精细化感兴趣区域选择<br>Step 3: integrate CNN classification 集成CNN分类<br>output: improved marking identification 改进的标记识别 |


## Newly Found Papers on ...
(Older entries get replaced automatically when the script runs again.)